{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZallfE2z9_dA"
      },
      "outputs": [],
      "source": [
        "# importing packages & libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading and processing data\n",
        "data = open('irish-lyrics.txt').read()\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "metadata": {
        "id": "ZRcJzW5E-DSk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1Nc5nlF-F9o",
        "outputId": "1e3fe185-c30d-4a74-a0db-666b2cbf46e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing data\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# padding sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# creating predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "fH73AYaO-HW2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
        "    tf.keras.layers.Bidirectional(LSTM(150)),\n",
        "    tf.keras.layers.Dense(total_words, activation = 'softmax'),\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= 'adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "b19UXQQp-JDL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training model\n",
        "history = model.fit(xs, ys, epochs=10, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rerQUIMH-KmZ",
        "outputId": "7a3a56dc-e0da-4c50-bff9-b47f50fd6b4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "377/377 [==============================] - 30s 70ms/step - loss: 6.7303 - accuracy: 0.0652\n",
            "Epoch 2/10\n",
            "377/377 [==============================] - 26s 69ms/step - loss: 6.2314 - accuracy: 0.0781\n",
            "Epoch 3/10\n",
            "377/377 [==============================] - 26s 70ms/step - loss: 5.9626 - accuracy: 0.0860\n",
            "Epoch 4/10\n",
            "377/377 [==============================] - 27s 71ms/step - loss: 5.6671 - accuracy: 0.0999\n",
            "Epoch 5/10\n",
            "377/377 [==============================] - 27s 70ms/step - loss: 5.3404 - accuracy: 0.1141\n",
            "Epoch 6/10\n",
            "377/377 [==============================] - 26s 70ms/step - loss: 4.9927 - accuracy: 0.1318\n",
            "Epoch 7/10\n",
            "377/377 [==============================] - 27s 71ms/step - loss: 4.6350 - accuracy: 0.1497\n",
            "Epoch 8/10\n",
            "377/377 [==============================] - 27s 71ms/step - loss: 4.2716 - accuracy: 0.1784\n",
            "Epoch 9/10\n",
            "377/377 [==============================] - 27s 71ms/step - loss: 3.9071 - accuracy: 0.2173\n",
            "Epoch 10/10\n",
            "377/377 [==============================] - 26s 70ms/step - loss: 3.5481 - accuracy: 0.2702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating text\n",
        "seed_text = \"I too am a human being and I\"\n",
        "# seed_text = \"I've got a bad feeling about this\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted.argmax():\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpn4x0x-MBB",
        "outputId": "39fbfd99-9438-44a4-d97b-10fbdaef3bc6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I too am a human being and I suppose been a eyes of killarney no more more more rest no more died and shrill dhu bubblin in dove and stainless you to the bower of the summer trace dwell loud and shrill dhu dhu stainless you away me from lip and bubblin my heart did strength sat down by the bridge of toome today today by toome today today by tory weeping and stainless bubblin bubblin bubblin bubblin my bride and dove and dhu and dhu for my fathers sportin died for strength gowns say me heartfrom day to the covers trace bubblin bubblin lassies today by side\n"
          ]
        }
      ]
    }
  ]
}